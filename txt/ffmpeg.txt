AVFrame
.data[8]，8路解码数据，如YUV用3路，7.1声道用8路
.linesize[8]，每路大小, audio 2路好像加一起写在 linesize[0]里

AVFrame 强转为void* 进入相应解码函数，进入后又强转为 AVFrame


libavformat -> AVPacket -> libavcodec  -> AVFrame

struct AVCodec {
	int (*encode2)(AVCodecContext *, AVPacket *, AVFrame *, int *got_packet_ptr);
	int (*decode) (AVCodecContext *, void *outdata, int *outdata_size, AVPacket *);
};


extended_data



AudioChannelManipulation

audio_thread
decoder_decode_frame
avcodec_decode_audio4
libopus_decode 具体音频解码器的解码函数

ffplay:
强制解码器，不认 -c:a
-codec:v or a  codec_name
-acodec codec_name
-vcodec codec_name




https://trac.ffmpeg.org/wiki/AudioChannelManipulation

https://trac.ffmpeg.org/wiki/Encode/AAC

https://trac.ffmpeg.org/wiki/StreamingGuide


Vitamin: av_log 对接 av_log_set_callback()

avcodec_encode_video(.., uint8_t *buf,..., const AVFrame *pict);
avcodec_default_get_buffer2() //获取AVFrame 中3通道的buffer
stream_index 字段判断音频/视频

graph2dot

https://trac.ffmpeg.org/wiki //用trac 维护wiki和trace


EasyICE 分析hls

-f 文件扩展名无效

ffplay 只播放一个声道

作者 bellard.org

ffmpeg -i input.mp4 output.avi //最简
ffplay -h filter=codecview  查看某个filter的帮助
"Filtering and streamcopy cannot be used together"

-录制屏幕
ffmpeg -f fbdev -r 10 -i /dev/fb0 out.avi //抓不到，ubuntu没有用fb

http://blog.csdn.net/leixiaohua1020/article/details/11729929
http://forum.doom9.org/ 专业视频论坛


1 compile

libsdl1.2-dev
yasm

x86/x86-64汇编/反汇编器yasm重写自NASM，(为啥不用gcc的as？)
--disable-yasm  disable use of nasm/yasm assembly
--disable-asm   disable all assembly optimizations

不加--disable-asm 进入get_cabac_inline_x86 加进入 get_cabac_inline （与--disable-yasm无关）

基本区别是：
.C或.h中内嵌的汇编__asm__("..") 用gcc asm编译
单独的.s/.asm 文件用yasm编译

make -j8
make V=1 (显示详情) 
configure/Makefile手写的

libray.mak/common.mak/config.mak(./configure)
Makefile 开头包含 common.mak config.mak 靠：
$(foreach D,$(FFLIBS),$(eval $(call DOSUBDIR,lib$(D))))包含 sub-Makefile（包含config.mak）和library.mak（包含common.mak）

--disable-optimizations  disable compiler optimizations  (gdb)
configure（搜optimizations）无-O3，否则有，在_type=gcc，_cflags_noopt=没有定义，其他编译器_cflags_noopt=O1

--enable-libx264 (libx264-dev) --enable-gpl

--enable-pic (config.mak多处产生PIC字样)

--disable-dct     disable DCT code
--disable-dwt     disable DWT code

--disable-programs
--disable-ffplay
--disable-programs //ffplay ffmpeg etc
--enable-libv4l2   enable libv4l2/v4l-utils [no]

libx264(vlc):
源码make install，主要是/usr/local/lib/libx264.so

aac:自带和libfaac (FAAC : Freeware Advanced Audio Coder)

码率：
旧: -b/-ab  新: -b:v / -b:a 
-b is ambiguous
ffmpeg_opt.c opt_bitrate()

-c:v libx264/h264/libopenh264

libavcodec/libx264.c

--enable-libopenh264

.video_codec = CONFIG_LIBX264_ENCODER ? AV_CODEC_ID_H264 : AV_CODEC_ID_MPEG4
//没有264编码就用mpeg4


2 Libavfilter (filter)
Libavfilter : graph-based frame editing library
lavfi : Libavfilter input virtual device, reads data from the open output pads of a libavfilter filtergraph. 

ffplay -f lavfi -i 'movie=/dev/video0:v4l2'  //-i 可省
ffplay -f lavfi 'movie=/dev/video0,edgedetect=low=0.1:high=0.4'  //edgedetect  
ffplay -f lavfi 'movie=why.mp4,ocv=filter_name=smooth:gaussian|49'  //高斯模糊
ffplay -f lavfi 'movie=why.mp4,scale=960x540,split[1][out1];[1]smartblur=luma_radius=5' //2路视频可切换
ffplay -f lavfi 'movie=x.mp4,rotate=t/10' //视频旋转  t/n是filter变量，n帧数t时间(s)
ffplay -f lavfi 'amovie=x.mp3,showcqt' //显示cqt频谱
ffplay -f lavfi 'amovie=/dev/dsp1:oss,showcqt' // mic cqt频谱(oss驱动)
ffplay -f lavfi "amovie=\'hw:0\':alsa,showcqt" //alsa 声卡 冒号要转义 或：
ffplay -f lavfi "amovie=hw\':\'0:alsa,showcqt" //关键是hw和0之间的冒号要转义
ffplay -f lavfi "amovie=\'hw:1\':alsa,showcqt" //usb mic
ffplay -f lavfi 'aevalsrc=sin(440*2*PI*t)/8,asplit[a][out1];[a]showcqt[out0]' //显示并发声 /8降低发声幅度 out0可不写
ffplay -f lavfi 'aevalsrc=sin(440*2*PI*t)+sin(440*4*PI*t), showcqt' //叠加谐波
ffplay -f lavfi 'sine=f=17*1000,asplit[a][out0];[a]showspectrum[out1]' //17KHz 听不见
ffplay -f lavfi 'sine=f=440,asplit=4[1][2][3][out0];[1]showcqt[out1];[2]showspectrum[out2];[3]showwaves[out3]' //v切换3视频
ffplay -f lavfi -graph_file  1.txt // 1.txt:movie=why.mp4[1];movie=1.png,[1]overlay
ffplay -f lavfi 'aevalsrc=0.1*sin(2*PI*(360-2.5/2)*t) | 0.1*sin(2*PI*(360+2.5/2)*t),asplit[a][out1];[a]showcqt[out0]' //立体声

ffmpeg -i why.ts  -vf 'split[1][2];[2]histogram,[1]overlay'  why1.ts //ok
ffmpeg -i why.ts  -filter 'split[1][2];[2]histogram,[1]overlay'  why1.ts  //fail! "Cannot connect video filter to audio input"
ffmpeg -i why.ts  -filter:v 'split[1][2];[2]histogram,[1]overlay'  why1.ts //ok!


//默认输出采样率44100，-ar 8000可改变，不能放在-i前，提示Option sample_rate not found，filter上来的数据采样率是固定的
ffmpeg -re -t 10 -f lavfi -i 'aevalsrc=sin(17000*2*PI*t)+sin(18000*2*PI*t)' (-ar 8000) 17-18.wav

//播放发现频谱很多，不单80和400，因相加超出范围，出现很多0x7FFF,即“方波”，各分量衰减0.4正常
ffmpeg -re -t 10 -f lavfi -i 'aevalsrc=sin(80*2*PI*t)+sin(400*2*PI*t)' -ar 8000 80-400.wav




-----------
filter:
分为 Video Filters / Audio Filters / Multimedia Filters

delogo 用于去logo或台标
fftfilt
vflip
hflip

setpts  works on video frames 
asetpts works on audio frames

Audio Filters:
atempo : Adjust audio tempo. an implementation of WSOLA (similar to SOLA) algorithm 
波形相似叠加算法 waveform similarity overlap and add
[ˈtempəʊ] [乐] 速度，拍子; 速度

asetrate

分 simple（-filter 单进单出)和complex(多进多出)
-af = -filter:a
-vf = -filter:v
ffplay 用 -vf -af
ffmpeg 用 -filter[:stream_specifier] or vf/af

ffplay -vf setpts=PTS/2 why.mp4 //视频2倍速，音频正常 
ffplay -vf setpts=PTS*2 why.mp4 //视频1/2速，音频正常
ffplay -vf setpts=PTS/2 -af asetpts=PTS/2 -i why.mp4 //正常播放
ffplay -vf setpts=PTS/2 -af atempo=2 -i why.mp4 //音视频2倍速
ffplay -vf setpts=PTS/2 -af asetrate=r=88200 -i why.mp4// 都是2倍速，音频效果完全不同  
atempo将速度变快

调整pts不会影响音频的播放，它就是以44.1k采样频率播放（与xuss一起理解到的）  
setpts=PTS-STARTPTS


filtergraph

-dumpgraph 1  (仅lavfi时有效)

split  asplit  视频分割  音频分割

滤波器类型：
overlay 2个输入，第1个是main（即底）

同一路径逗号分隔, 不同路径分号分隔
showcqt:
fps:Default 25
count:每帧fft变换次数，默认6。Note: audio data rate must be divisible by fps*count.?

下面一半镜像上面一半
ffmpeg -i INPUT -vf "split[1][2]; [2]crop=iw:ih/2:0:0, vflip[3];[1][3]overlay=0:H/2" OUTPUT

ffplay why.mp4 -vf 'split[1][2];[2]crop=iw:ih/2:0:0,vflip,[1]overlay=0:H/2'

//下面overlay V2.3/V2.5正常，V2.4卡死
ffplay -f lavfi 'movie=why.mp4[1];movie=1.png,[1]overlay' -dumpgraph 1
ffplay why.mp4 -vf 'movie=1.png,[in]overlay'
ffplay why.mp4 -vf 'movie=1.png[1],[in][1]overlay' //效果同上，上一级输出默认就到了第二个参数的位置
ffplay why.mp4 -vf 'movie=1.png[1],[1][in]overlay' //小png在下即main，整个视频显示很小一部分


ffplay -f lavfi "movie=why.mp4"  //只有视频
ffplay -f lavfi "amovie=why.mp4" //只有声音


//2视频合并，可能需要同步，所以需要一致化pts
ffplay /dev/video0 -vf 'movie=why.mp4,scale=480x270,setpts=PTS-STARTPTS[1];[in]setpts=PTS-STARTPTS[in1],[in1][1]overlay'

原来在跟目录有 version.h : #define FFMPEG_VERSION "2.0"

ffserver	  HTTP串流server，支持时移
libavcodec	  编解码库
libavutil	  工具函数库
libpostproc	  前期处理库
libavdevice   设备如alsa\v4l2\fbdev(framebuffer)\sdl\grab(攫取，意思是抓屏吧)
libswscale	  Color conversion and scaling
libswresample Audio resampling, sample format conversion and mixing

-strict -2  <=> -strict experimental

very  2
strict 1
norma 0
unofficial -1 
experimental -2


2 参数
ffplay可省略 -i，ffmpeg不可
查看能力
-devices
-codecs
-filters
-pix_fmts
-sample_fmts
-formats 
-f fmt (input/output) force format, -formats 显示的格式都可以指定
-an no audio
-s size Set frame size (WxH or W*H or abbreviation如pal/ntsc/cif)
video_size_abbrs数组定义各种abbr如  qcif:176x144, cif:352x288, qqvga: 160x120, vga:640x480
打印：Option -s is deprecated, use -video_size.（没有-s简洁^_^）

-ac 音频通道数
-ar 采样率（可以重新采样，如把44.1k降低到8k）
-fs 控制文件输出的大小 file size
-vf 没有comma，semecolon时可不加单/双引号
video4linux2 = v4l2  最新版本不需要 -f v4l2

3 命令

ffplay why.mp4 -vismv bf //显示mv,deprecated(显示不出因为why.mp4没有b帧)
ffplay -vf codecview=mv=pf+bf+bb -flags2 export_mvs input.ts //显示运动矢量 pf:p帧 bf:b帧 bb:backward b
ffplay -f v4l2 /dev/video0 -vf histogram //直方图
ffplay -f v4l2 /dev/video0 -vf 'split[1][2];[2]histogram,[1]overlay' //直方图+视频

普通滤波器只能1路输入，而filter_complex 可以多路输入

libavcodec 的参数
-debug pict
-debug mb_type
libavformat 的参数
-fdebug ts （time stamp 即FF_FDEBUG_TS 不是ts流)，打印pts dts 和包大小  

ffplay why.mp4 -debug mb_type  打印宏块类型 -debug支持很多选项

c270 camera：
ffplay /dev/video0  //default:yuyv422,640x480,147456kb/s (640x480x16x30=147456k)
ffmpeg -i /dev/video0 -c:v h264 -f h264 udp://ip
ffmpeg -f alsa -i hw:1 -f v4l2 -i /dev/video0 out.mpg

ffplay udp://192.168.6.20:1000 
ffplay 播放ts小片，提示 non-existing SPS 0 referenced in buffering period

wav splite头可得pcm
ffmpeg -i f.wav -f s16le -acodec pcm_s16le f.pcm  //ff不认.pcm
若f.wav 已是pcm_s16le：
ffmpeg -i f.wav -f s16le -c:a copy f.pcm //去头
ffmpeg -i f.wav -f s16le (-c:a copy) -ar 8000 -ac 1 f.pcm //变为8k采样，单通道
加括号里的内容，就直接拷贝，重采样和单通道参数无效

ffplay f.pcm //无法播放
ffplay f.pcm -f s16le // 默认mono，44100，必要时 -ar 8000 / -channels 2

支持：
s16be  PCM signed 16-bit big-endian
s16le  PCM signed 16-bit little-endian
s24be  PCM signed 24-bit big-endian
s24le  PCM signed 24-bit little-endian
s32be  PCM signed 32-bit big-endian
s32le  PCM signed 32-bit little-endian
s8     PCM signed 8-bit

alsa:
ffmpeg -f alsa -i hw:0 o.wav //声卡ok

取usb摄像头音频数据：
ffmpeg -channels 1 -f alsa -i hw:1 o.wav //usb mic 提示：
不加-channels 1 默认2通道:
cannot set channel count to 2 (Invalid argument)
alsa : snd_pcm_hw_params_set_channels返回错误


ffmpeg -re -i why.mp4 -f sdl ddd -f alsa default //直接输出视频和音频

ffprobe -i xx.mp4 查看信息
ffprobe -show_packets

-lavfi = -filter_complex
这2个参数是给ffmpeg 用的，而不是ffplay
//4x4
ffmpeg -i why.mp4 -filter_complex 'scale=480x270,split=16[0][1][2][3][4][5][6][7][8][9][10][11][12][13][14][15];nullsrc=s=1920x1080[b];[0]ocv=filter_name=smooth:gaussian|49,[b]overlay[o0];[o0][1]overlay=x=480[o1];[o1][2]overlay=x=960[o2];[o2][3]overlay=x=1440[o3];[o3][4]overlay=y=270[o4];[o4][5]overlay=y=270:x=480[o5];[o5][6]overlay=y=270:x=960[o6];[o6][7]overlay=y=270:x=1440[o7];[o7][8]overlay=y=540[o8];[o8][9]overlay=y=540:x=480[o9];[o9][10]overlay=y=540:x=960[o10];[o10][11]overlay=y=540:x=1440[o11];[o11][12]overlay=y=810[o12];[o12][13]overlay=y=810:x=480[o13];[o13][14]overlay=y=810:x=960[o14];[o14][15]overlay=y=810:x=1440' why-4.ts
//因nullsrc无限长，不会终止，改为 nullsrc=s=1920x1080:d=205
源码中d参数默认-1，表示无限长

2x2马赛克：
ffmpeg -i why.mp4 -i why.mp4 -i why.mp4 -i why.mp4 -lavfi 'nullsrc=size=1920x1080[b];[0:v]scale=960x540[0];[1:v]scale=960x540[1];[2:v]scale=960x540[2];[3:v]scale=960x540[3];[b][0]overlay[o1];[o1][1]overlay=x=960[o2];[o2][2]overlay=y=540[o3];[o3][3]overlay=x=960:y=540' why-4.ts

简化标签：
ffmpeg -i why.mp4 -i why.mp4 -i why.mp4 -i why.mp4 -lavfi 'nullsrc=size=1920x1080[b];[0:v]scale=960x540,[b]overlay[o1];[1:v]scale=960x540,[o1]overlay=x=960[o2];[2:v]scale=960x540,[o2]overlay=y=540[o3];[3:v]scale=960x540,[o3]overlay=x=960:y=540' why-4.ts
或更简单^_^：//必需split
ffmpeg -i why.mp4 -lavfi 'scale=480x270,split=16[0][1][2][3][4][5][6][7][8][9][10][11][12][13][14][15];nullsrc=size=1920x1080[b];[b][0]overlay[o0];[o0][1]overlay=x=480[o1];[o1][2]overlay=x=960[o2];[o2][3]overlay=x=1440[o3];[o3][4]overlay=y=270[o4];[o4][5]overlay=y=270:x=480[o5];[o5][6]overlay=y=270:x=960[o6];[o6][7]overlay=y=270:x=1440[o7];[o7][8]overlay=y=540[o8];[o8][9]overlay=y=540:x=480[o9];[o9][10]overlay=y=540:x=960[o10];[o10][11]overlay=y=540:x=1440[o11];[o11][12]overlay=y=810[o12];[o12][13]overlay=y=810:x=480[o13];[o13][14]overlay=y=810:x=960[o14];[o14][15]overlay=y=810:x=1440' why-4.ts

ffplay why.mp4 -vf 'scale=960x540,split[1][out1];[1]smartblur=luma_radius=5' //fail
ffplay why.mp4 -vf 'scale=960x540,smartblur=luma_radius=5' //ok
ffmpeg -i why.mp4 -f mpegts - | ffplay - //边转边播

ts转mp4
ffmpeg -i xx.ts -c copy xx.mp4

ffmpeg -i x.mp4 -c:v h264 -c:a copy output.mp4
ffmpeg -i xx.ts -codec copy xx.mp4
ffmpeg -i xx.ts -c copy xx.mp4

-c:v(不能用在ffplay) == -vcodec == -codec:v
-ss position (input/output)  即可用于输入又可用于输出

"When used as an input option (before -i)"
"When used as an output option (before an output filename)"


ffmpeg -ss 30 -t 30 //视频截取
ffmpeg -i xx.ts -c:v copy -f rawvideo xx.es //提取es
ffmpeg -i udp://ip:port -c:v h264 -c:a aac -ar 48000 -ac 2 -strict -2 -f mpegts udp://ip:port
ffmpeg -i xx.m4a out.wav //alac格式.m4a转wav
ffmpeg -i 1.jpg -pix_fmt bgra 1.bmp //32bit bmp
ffmpeg -i x.mp4 x.ts //默认v:mpeg2 main + a:mpeg2，码率低不清晰
ffmpeg -i x.mp4 -b:v 12000k x.ts //mp4 h264 6M码率，转为mpeg2基本双倍就清晰了
ffmpeg -i INPUT -vf '[in]select=eq(pict_type\,I)' I.frames.ts //提取I帧
ffmpeg -i INPUT -vf '[in]select=eq(pict_type\,I),showinfo[out]' I%03d.jpeg //提取I帧
ffmpeg -i INPUT -vf select="eq(pict_type\,PICT_TYPE_I)" -vsync 0 frame%03d.jpg
ffmpeg -i INPUT -vf %04d.jpg //所有帧
图片转视频：
ffmpeg -r 24 -i %04d.jpg test.mp4


-qp (quantification parameter) 用于控制码率，默认20左右

/ffplay xx -debug pict  可以看到每帧的qp，还有IDR帧的间隔

#camera叠加透明视频，tail.mp4 Y分量扩展成alpha分量实现透明，即tail.mp4亮度越暗，像素越透明
ffplay -f v4l2 -i /dev/video0 -vf 'movie=filename=tail.mp4:loop=200:stream_index=0,
setpts=N/(25*TB),split[1][2],alphamerge[3];[main][3]overlay=main_w-overlay_w-480:0'

.mp4默认用mpeg4 Simple Profile
-vbsf h264_mp4toannexb
-absf 
-sbsf

添加logo
ffplay /dev/video0 -vf 'movie=1.png[1];[in][1]overlay=main_w-overlay_w-100:0[out]'
main_w-overlay_w-100:0:1或者main_w-overlay_w-100:0:0 是否透明

ffmpeg -i xx.ts -map p:304 -c:v h264 -x264-params slices=5 x1.ts

xx.ts用-map p:304选择节目
x264中其他参数：
 -I, --keyint Maximum GOP size [250]  //相当于哈雷转码器的N
 -b, --bframes <integer> Number of B-frames between I and P [3] //相当于哈雷的M，为0只有I和P帧
 --no-cabac Disable CABAC

hls分片：
m3u8-segmenter (用ffmpeg里面的hlsenc.c制作)

sub_packet_size is invalid
一个rm视频，用视频截断出来新的rm，播放或者fprobe都提示sub_packet_size is invalid，然后终止运行.

-v :
log.h : AV_LOG_QUIET / AV_LOG_PANIC

很多选项支持[:stream_specifier]

用pins或pads描述各级滤波器的连接点，输出不写默认为out0
asplit=3 //默认2

buffersrc  buffersink 理论上任何命令都有这2个filter，一般filter 有I/O，这2个filter只有I或O
tools/zmpsend  zmq 给 ffmpeg filter发消息

av_rescale_rnd：计算 "a * b / c" 并分五种方式取整，用于将时钟基准 c 表示的数值 a 转换成以时钟基准b表示

tb:time base
tbr: 帧率
tbc: 视频层（st->codec）的时间精度
tbn: 文件层（st）的时间精度

tbn :time base in AVStream that has come from the container. It is used for all AVStream time stamps.
tbc :time base in AVCodecContext for the codec used for a particular stream. It is used for all AVCodecContext and related time stamps.
tbr :guessed from the video stream and is the value users want to see when they look for the video frame rate, 
except sometimes it is twice what one would expect because of field rate versus frame rate. 

多码率hls类似多节目ts，打印：
Program 0
Program 1
Program 2

ffmpeg2.0.2播m3u8,Duration统计有误，hls.c hlsproto.c中duration定义为int改float

seek时进入hls.c中hls_read_seek()
live和dvr 不停获取m3u8文件
 
hls vod时 执行seek的流程
ffplay.c -> read_thread -> [if (is->seek_req)处的]
avformat_seek_file -> av_seek_frame（utils.c中） -> seek_frame_internal -> read_seek

主要是修改 hls_read_seek 这个函数

version 2.0.2
hls_read_header 里

v->cur_seq_no = v->start_seq_no + v->n_segments - 3;
v->start_seq_no等于m3u8文件第一行的ts编号，v->n_segments 是ts行数，比如cctv2运行若干
时间后 start_seq_no = 125202 n_segments = 468


hls.c 与 hlsproto.c 按文件格式和按网络协议对待

s->streams[stream_index]->time_base.den  等于 90000。 90kHz 是视频领域的常量
avctx->time_base.den = 90000;

stream_index 是什么含义？
int seek_frame_internal(AVFormatContext *s, int stream_index, int64_t timestamp, int flags)
hls 点播wowza时候 stream_index == -1

if (s->iformat->read_seek || 1) {//gaojie
    int dir = (ts - (uint64_t)min_ts > (uint64_t)max_ts - ts ? AVSEEK_FLAG_BACKWARD : 0);
    int ret = av_seek_frame(s, stream_index, ts, flags | dir);//vod只走这个下面条件不满足
    if (ret<0 && ts != min_ts && max_ts != ts) {
        ret = av_seek_frame(s, stream_index, dir ? max_ts : min_ts, flags | dir);
        if (ret >= 0){
            ret = av_seek_frame(s, stream_index, ts, flags | (dir^AVSEEK_FLAG_BACKWARD));
        }
    }
    return ret;
}

seek位置的设置：
ffplay.c中 event_loop -> case SDL_MOUSEMOTION
ffplay.c p3250 附近
mpegts.c完成dvb解析

libavformat/utils.c

previous_duration 是vitomio加的，增加EXT-X-DISCONTINUITY的解析（用于视频时间过长或片头广告）

hls.c 和 hlsproto.c中各有一处 parse_playlist  ，hlsproto.c 好像没有使用

-mpegts解析---
mpegts_read_packet -> handle_packets -> read_packet & handle_packet


enum AVPictureType {
    AV_PICTURE_TYPE_NONE = 0, ///< Undefined
    AV_PICTURE_TYPE_I,     ///< Intra
    AV_PICTURE_TYPE_P,     ///< Predicted
    AV_PICTURE_TYPE_B,     ///< Bi-dir predicted
    ...
};

直播信号AVFormatContext -> start_time(pts)
测了几个点播节目，这个值固定是1s 即 1000000

seek时，为啥要加 pts(即start_time)呢？

android-ffmpeg 
hls_seek 中的pos 值会随时间推移增加

Duration: 00:19:58.08, start: 45461.560667 (PTS)  (dump.c)
进入seek的时候，取出的 pos 就是这个“start” 并且 pos的值不会更新，进入只赋值1次。

VideoState 保存着各种状态
ffplay 当前时间应该是读取的pts

把服务器dvr停止，此时直播变点播。
start: 51746.360667  虽然start不从0（或说是纯点播节目的10.0000）开始，但进度条是正确的
说明只要把pts解正确就没有问题了。

ffplay AV_SYNC_AUDIO_MASTER

seek的时候，会先调用stop在调用start

onSeekComplete
hls_read_header

1.移植了ffmpeg并将与媒体相关的结构体在java层重新进行了封装，方便应用程序在java层直接操作ffmpeg API,如各种媒体格式转码及播放
2.模仿Android的MediaPlayer类实现了ffmpeg的播放接口，如setDataSource(),setDisplay(),start(), stop(),pause()等，缺点是没有实现seek功能。

ffplay 回退的时候非常快，怎么做到的
http://192.168.7.35:1935/dvr/cctv2.stream/playlist.m3u8?DVR: error while seeking

按键控制seek 走event_loop处的do_seek: 调用stream_seek 指定rel参数
点击屏幕位置时，seek_rel 为0  
上下左右键时，分别是 60000000 -60000000 -10000000 10000000
点击屏幕取值 
INT64_MIN  -9223372036854775808 = 0x8000000000000000
INT64_MAX  9223372036854775807 = 0x7fffffffffffffff
点击屏幕 seek_flags = 0


read_seek2 不成立！
avformat_seek_file 里面走 if (s->iformat->read_seek || 1)

seek到最后时 read_thread 不运行了

/**
 * Presentation timestamp in AVStream->time_base units; the time at which
 * the decompressed packet will be presented to the user.
 * Can be AV_NOPTS_VALUE if it is not stored in the file.
 * pts MUST be larger or equal to dts as presentation cannot happen before
 * decompression, unless one wants to view hex dumps. Some formats misuse
 * the terms dts and pts/cts to mean something different. Such timestamps
 * must be converted to true pts/dts before they are stored in AVPacket.
 */
int64_t pts;
/**
 * Decompression timestamp in AVStream->time_base units; the time at which
 * the packet is decompressed.
 * Can be AV_NOPTS_VALUE if it is not stored in the file.
 */
int64_t dts;


/**
 * Decoding: pts of the first frame of the stream in presentation order, in stream time base.
 * Only set this if you are absolutely 100% sure that the value you set
 * it to really is the pts of the first frame.
 * This may be undefined (AV_NOPTS_VALUE).
 * @note The ASF header does NOT contain a correct start_time the ASF
 * demuxer must NOT set this.
 */
int64_t start_time;

##pts_drift=-1383549524.89 update=1383556510.83 6985.93

时钟更新流程：周期调用
update_video_pts -> set_clock -> set_clock_at

// reference time stamp should be 60 s before first time stamp
pts_wrap_reference 是 first dts - 60s

时移时间M分钟，过N分钟，N（N<M）分钟之内不可timeshift，M分钟后可随意跳转

开始后，有大约1分钟的地方不能timeshift，过2分钟后，就3分钟的位置不能看，过10分钟就11分钟
的位置不能看依次类推，但是开始超过20分钟，就可以随意时移了。
将开始的pts向后平移 duration 时间，解决了问题。

int update_wrap_reference(AVFormatContext *s, AVStream *st, int stream_index)
{
// reference time stamp should be 60 s before first time stamp
int64_t pts_wrap_reference = st->first_dts - 
av_rescale(60, st->time_base.den, st->time_base.num)-1200*90000; //后退1200s
//这里减了能保证ffplay下方显示的pts正常，因为可以不进入 wrap_timestamp()的 +(1ULL<<st->pts_wrap_bits)逻辑了
不减时间也可以，对于android版本
注意，修改时不能影响到点播。果然影响了点播，点播seek后会自动向前移动20分钟，:-)

airplay 将mp3转换成alac再发送，java版alac解析出的pcm提示：
Format mp3 detected only with low score of 25, misdetection possible

某问题流用mediainfo看仅30s，ffplay播放不了，把中间错误的删除即可

原来保存的文件用ffplay播放速度变为1半，是因为，每个通道的16bit 都变成了32bit，并且做
了符号扩展，所以每16bit前面都添加了 0xffff 或者 0x0000。这同时解释了速度减半和一个声道没声音2个问题。

时移设置6小时，每条2.4s，共8100条m3u8 分析这个文件需要4s而一条2.4s，分析时间比更新时间长，出问题。

vitamio 继续深化ffmpeg，作出8秒视频的应用。开源/商业2条线

先用cheese再用ffplay，格式变为：yuyv422, 1280x960, 589824 kb/s, 30 fps

cheese webcam booth软件(软件中心搜cheese)

ubuntu11.10配置sdl时--disable-oss(因无oss驱动，用alsa)

加-ggdb编译参数：
--enable-debug=gdb 设置 -g1 -g2 或 -g3 即-glevel


--makefile分析
ECHO   = printf "$(1)\t%s\n" $(2)

默认静态链接
--enable-shared   build shared libraries [no]

vlc录制的ts流无法算出总时间，ffplay无法快进
flv与mp4类似

ffmpeg seek要清buffer重新下载数据。

buffer缓冲通知机制
Vplayer播放harmonic很好

libavformat/http.c
因为仅使用POST部分功能，所以没有完全实现它。
MAX_REDIRECTS 8

ts计算时长:头尾pts相减
ffplay -v 48 http://ip/xxx.tx可见取头尾操作

http部分

URLProtocol

结构体引用结构体指针，理解为聚合或组合关系，直接把结构体放在首位引入，理解成继承关系

libavformat/mov.c : RAW MPEG-4 video demuxer

AVInputFormat *iformat; // The input container format

在各自文件格式里定义：
AVInputFormat ff_mov_demuxer

AVClass av_format_context_class 仅在libavformat/options.c中定义


typedef struct AVBufferRef {
    AVBuffer *buffer;
    uint8_t *data;
} AVBufferRef;

struct AVBuffer {
    uint8_t *data;   //2结构体，data指向同一空间
}

(gdb) p /x *pkt->data@20
00 00 01 a3 65 88 80 80 17 e3 85 1c  //第一个视频帧 000001a3 是这个帧的大小

解一帧数据：
avcodec_decode_video2  返回AVPacket里面数据的大小（在AVPacket结构体里也有这个字段），<0错误

h264_decode_frame (why.mp4 AVPacket.data 有00 00 01 a3 4byte)

utils.c中还可能从ff_thread_decode_frame进入h264_decode_frame，一回事，只是加入了线程


注意264文档中的NumBytesInNALunit

libavformat/utils.c 定义框架：


AVFormatContext{ //Format I/O context
    /* - demuxing: set by avformat_open_input()*/
    void *priv_data; //理解无处不在的priv_data
	//
	int64_t start_time;
    /*AV_TIME_BASE基准(即us)流长度，一般由AVStream values推导出来，Demuxing时由libavformat设置*/
    int64_t duration;
}
avformat_alloc_context(void){ 创建 AVFormatContext
s->av_class = &av_format_context_class;
}

播放时间如何上报，buffer状态上报

libavformat/utils.c 是框架层，而我原来修改了框架层，这是不对的。
hls.c hlsprot.c 用2种视角实现hls，前者当成demux，后者当场protocol


s: AVFormatContext
st: AVStream
pkt: AVPacket

start_time何时赋值？

mp4文件初始化start_time:

update_initial_timestamps() 的

    if (st->start_time == AV_NOPTS_VALUE)
        st->start_time = pts;

update_initial_timestamps  频繁引入，但几乎在开始处判断退出(正常播放以及seek都不会触发)：

if (st->first_dts != AV_NOPTS_VALUE || dts == AV_NOPTS_VALUE ||st->cur_dts == AV_NOPTS_VALUE || is_relative(dts))
        return;


st->first_dts ??


is_relative(dts)


INT64_MAX  2^63-1 = 9222809086901354495

INT64_MAX = 9223372036854775807
RELATIVE_TS_BASE = 9223090561878065151
RELATIVE_TS_BASE - (1LL<<48) = 9222809086901354495

libavformat/utils.c:
#0  update_initial_timestamps (频繁进入)
#1  compute_pkt_fields
#2  read_frame_internal
#3  avformat_find_stream_info
#4  read_thread

#0  update_initial_timestamps
#1  compute_pkt_fields
#2  read_frame_internal
#3  av_read_frame
#4  read_thread


#define RELATIVE_TS_BASE (INT64_MAX - (1LL<<48))

int is_relative(int64_t ts) {
    return ts > (RELATIVE_TS_BASE - (1LL<<48));
}

AVOutputFormat {
int (*write_header)(AVFormatContext *);
int (*write_packet)(AVFormatContext *, AVPacket *);
int (*write_trailer)(AVFormatContext *); //??
}

AVInputFormat {
int (*read_header)(AVFormatContext *);
int (*read_packet)(AVFormatContext *, AVPacket *);
int (*read_seek)(AVFormatContext *,...);
}

AVStream

AVPacket{ //demuxer到decoder或encoder到muxer，包含一个视频帧或几个音频帧
	AVBufferRef *buf; //reference-counted
	uint8_t *data; //压缩数据
}

AVBuffer  /*is an API for reference-counted data buffers*/

2个核心对象
AVBuffer ：描述数据buffer
AVBufferRef

./libavutil/avutil.h:
#define AV_TIME_BASE            1000000

#define AVSEEK_FLAG_BACKWARD 1 ///< seek backward
#define AVSEEK_FLAG_BYTE     2 ///< seeking based on position in bytes
#define AVSEEK_FLAG_ANY      4 ///< seek to any frame, even non-keyframes
#define AVSEEK_FLAG_FRAME    8 ///< seeking based on frame number


why.mp4(mov) seek到106s位置的过程：
#0  mov_read_seek (sample_time=106020, flags=1)  /*flags=1 代表 AVSEEK_FLAG_BACKWARD */
#1  seek_frame_internal (timestamp=106020, flags=1)
#2  av_seek_frame (timestamp=106020198, flags=1)
#3  avformat_seek_file (ts=106020198, flags=0) 

seek_frame_internal(..., int64_t timestamp, int flags)
{
//这里st->time_base.den == 1000  st->time_base.num == 1  //微秒变毫秒
timestamp = av_rescale(timestamp, st->time_base.den, AV_TIME_BASE * (int64_t) st->time_base.num);
}

ts文件AVSEEK_FLAG_BYTE标志设置，AVInputFormat ff_mpegps_demuxer 没有定义seek函数，seek_frame_internal 直接一字节为单位，如36M文件，seek 50%就是跳转到18M位置。
可能因为ts流是均匀混合的，时间和物理位置按比例对应。

libavformat 还包含 tcp.c tcp简单封装

URLProtocol类型：
ff_tcp_protocol
ff_udp_protocol
ff_http_protocol
ff_https_protocol
ff_ftp_protocol
ff_tls_protocol
ff_libsmbclient_protocol
ff_srtp_protocol
ff_crypto_protocol
ff_file_protocol (local file)
ff_pipe_protocol（管道输出作为数据来源）
ff_hls_protocol（协议方式处理的hls）
ff_cache_protocol

AVInputFormat ff_rtp_demuxer   rtp 当成muxer而不是protocol

回退发现vitamio获取currentpositiion 是pts最大值(26h30m43s) + 真正时间，如果是小-大，应该是pts最大值-真正时间。
原来，是检测到小-大，就认为pts一个循环，则pts最大值+真正时间。修改完成了hls 时移问题，最终getcurrentposition 还是不能用，直播获取当前时间本来没有意义。

hls下载一个ts分片启动独立tcp连接，抓包每切片完成，发送FIN

Bytestream IO Context
struct AVIOContext { //avio.h

}

avio.h 
aviobuf.c
从AVIOContext里读数据
int avio_r8(AVIOContext *s)
int avio_rl16(AVIOContext *s)

mov文件遇到stsd,调用 mov_read_stsd

network.c:
ff_socket()

系统回调机制：
系统各层面包含 AVIOInterruptCB 结构体，靠 ff_check_interrupt()函数调用回调函数。
ffplay.c 中注册的回调函数会周期性进入，播放http://ip/why.mp4调用顺序：

#0  decode_interrupt_cb
#1  ff_check_interrupt (在这里检查回调函数)
#2  retry_transfer_wrapper
#3  ffurl_read
#4  fill_buffer (本地文件没有这个)
#5  avio_read   (aviobuf.c)
#6  append_packet_chunked
#7  av_get_packet(sc->pb, pkt, sample->size) // obtain data !
#8  mov_read_packet (why.mp4)
#9  ff_read_packet
#10 read_frame_internal
#11 av_read_frame

检查ff_check_interrupt的地方有：
1 retry_transfer_wrapper
2 hls.c : read_data()

read file: 
#0  file_read (file.c or http_read(http.c))
#1  retry_transfer_wrapper
#2  ffurl_read
#3  avio_read
#4  append_packet_chunked
#5  av_get_packet
#6  mov_read_packet
#7  ff_read_packet
#8  read_frame_internal
#9  av_read_frame

一般（本地文件和http伪流）
ffio_fdopen()
{
 avio_alloc_context(ffurl_read, ...){
    s->read_packet = ffurl_read; 
 }
}

hls比较特殊
重新调了ffio_init_context(&pls->pb, pls->read_buffer, INITIAL_BUFFER_SIZE, 
	0, pls,read_data, NULL, NULL/*seek部分空*/); 
那seek就用自己实现的了

hls读playlist：
#0  http_read
#1  retry_transfer_wrapper
#2  ffurl_read
#3  fill_buffer
#4  avio_r8
#5  ff_get_line
#6  read_chomp_line
#7  parse_playlist
#8  hls_read_header
#9  avformat_open_input

读到具体视频数据时：

#0  http_read
#1  retry_transfer_wrapper
#2  ffurl_read
#3  read_from_url
#5  fill_buffer
#6  avio_read
#7  ffio_read_indirect
#8  read_packet
#9  handle_packets
#10 mpegts_read_packet
#11 ff_read_packet
#12 read_frame_internal
#13 av_read_frame
#14 hls_read_packet
#15 ff_read_packet
#16 read_frame_internal
#17 av_read_frame

播放http://ip/x.ts:
#0  http_read
#1  retry_transfer_wrapper
#2  ffurl_read
#3  fill_buffer
#4  avio_read
#5  ffio_read_indirect
#6  read_packet
#7  handle_packets
#8  mpegts_read_packet
#9  ff_read_packet
#10 read_frame_internal
#11 av_read_frame

http读mp4：
#0  http_read
#1  retry_transfer_wrapper
#2  ffurl_read
#3  avio_read
#4  append_packet_chunked
#5  av_get_packet
#6  mov_read_packet
#7  ff_read_packet
#8  read_frame_internal
#9  av_read_frame



1.txt :movie=why.mp4[1];movie=1.png[2];[1][2]overlay,nullsink
graph2dot -i 1.txt  -o tmp //tmp 是文本文件
dot -Tpng tmp -o 1.png //需要 graphviz包的 dot命令


make tools/graph2dot
编译测试代码：去掉#ifdef TEST make不行：multiple definition of `main'应该：
make testprogs

START_TIMER定义的前提是有HAVE_INLINE_ASM
直接播放yuv数组（不包含任何头信息）


ffmpeg 中 deinterlace是个filter，vlc也有反交错
--enable-gpl才能用:
1 boxblur filter
2 pp（postprocess  /libpostproc）
ffplay vlc-output.ts  -vf pp=lb
lb/linblenddeint Linear blend deinterlacing filter that deinterlaces the given block by filtering all lines with a (1 2 1) filter.
li/linipoldeint Linear interpolating deinterlacing filter that deinterlaces the given block by linearly interpolating every second line.
ci/cubicipoldeint Cubic interpolating deinterlacing filter deinterlaces the given block by cubically interpolating every second line.
md/mediandeint Median deinterlacing filter that deinterlaces the given block by applying a median filter to every second line.
fd/ffmpegdeint

ts AVPacket包含一个PES

PES: AUD + SPS + PPS + I slice (h.264没有 I frame的概念)  //一个 access unit
PES: AUD + P slice //P在B前解码，B后显示
PES: AUD + B slice

sidamingbu.ts pts dts:
帧率：25fps

B帧双向预测：“来自何方去往何处”

===默认参数
alsa_dec.c
AVOption options:
“channels {.i64 = 2}”
"sample_rate {.i64 = 48000}"

-map 参数  研究一下！

CLUT: Color LookUp Table
VDPAU (Video Decode and Presentation API for Unix)


global_quality

实际上FFmpeg的VBR和CBR都控制不好


有很多这个结构体如：
AVOutputFormat ff_au_muxer 但在源码里不会搜到第二个，他们是宏展开，在 allformats.c 中

int main()
{
  av_register_all(){
	REGISTER_MUXER(AC3, ac3);
  }
}

#define REGISTER_MUXER(X, x)                        \
{                                                   \
    extern AVOutputFormat ff_##x##_muxer;           \
    if (CONFIG_##X##_MUXER)                         \
        av_register_output_format(&ff_##x##_muxer); \
}

CONFIG_AC3_MUXER 根据configure 生成这个定义

polyphase filter 多相滤波器
grabbing devices 指的是音视频抓取设备，如摄像头，micphone

Anything found on the command line which cannot be interpreted as an option is considered to be an output filename. 
流类型：video/audio/subtitle/attachment/data

-map 选项看 Stream selection章

打印：
Filtering and streamcopy cannot be used together  因为滤波就是对解码后的内容进行处理

-qscale  怎么用

3段视频连接：
ffmpeg -i why.mp4 -i why.mp4 -i why.mp4 -lavfi '[0:0][0:1][1:0][1:1][2:0][2:1] concat=n=3:v=1:a=1' cat.mp4
对于音视频流单一情况，简化：
ffmpeg -i why.mp4 -i why.mp4 -i why.mp4 -lavfi 'concat=n=3' cat.mp4

这种有flag字样的参数，是1bit一个flag 后面的参数用累加方式如  
-flags2
-fflags

libavcodec/options_table.h
libavformat/options_table.h
ffmpeg_opt
---使用opencv
--enable-libopencv
ldd ffmpeg 发现连接了
/usr/local/lib/libopencv_core.so.2.4
/usr/local/lib/libopencv_imgproc.so.2.4

--分形
ffplay -f lavfi -i mandelbrot -vf "split=4[a][b][c][d],[d]histogram=display_mode=overlay:level_height=244[dd],[a]histogram=mode=waveform:waveform_mode=row:display_mode=overlay[aa],[b]histogram=mode=waveform:waveform_mode=column:display_mode=overlay[bb],[c]pad=iw+256:ih+256[cc],[cc][aa]overlay=x=W-256[ccc],[ccc][bb]overlay=y=H-256[x],[x][dd]overlay=y=H-256:x=W-256"

// 16:9 变 4:3
ffplay why.mp4  -vf 'pad=iw:3/4*iw:0:(oh-ih)/2:red'

网页上的文档是最新的，比如fftfilt 滤波器，v2.6都没有，要切换到master分支才有

libavutil/pixdesc.c pixdesc.h

IO_BUFFER_SIZE 32768 文件读，默认1次32k

ffurl_read
avio_read

#0  ffurl_read
#1  fill_buffer
#2  avio_read
#3  ffio_read_indirect
#4  read_packet
#5  handle_packets
#6  mpegts_read_packet
#7  ff_read_packet
#8  estimate_timings_from_pts
#9  estimate_timings
#10 avformat_find_stream_info
#11 read_thread

AVIOContext 有 read_packet  AVInputFormat 也有 read_packet  区别？

前者在avio_alloc_context 中传入
ffio_init_context

sample aspect ratio (SAR)
display aspect ratio (DAR)
信息：
Video: h264 (High) (HDMV / 0x564D4448), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], 23.98 fps, 23.98 tbr, 90k tbn, 47.95 tbc
在 ./libavcodec/utils.c avcodec_string()中，打印 AVCodecContext 的 codec_tag 字段，ts流来自PMT表，
如果有 Program info:字段就显示 “format_identifier” 即例中:HDMV
若没有 则显示Program elements:字段的 stream_type字段如：
Video: h264 (High) ([27][0][0][0] / 0x001B), yuv420p
27=0x1B 是H.264的 ISO 流类型

log系统

av_dlog
av_log

MP@HL Main Profile@High Level
MP@ML Main Profile@Main Level

-movflags rtphint 可以流化为dss可用

--
#0  write_hint_packets
#1  ff_mov_add_hinted_packet
#2  ff_mov_write_packet
#3  mov_write_single_packet
#4  mov_write_packet
#5  write_packet
#6  av_interleaved_write_frame
#7  write_frame

dts 大概40ms 一个间隔，这是24帧的节奏 (why.mp4)

movenchint.c

---
rtsp推流
ffmpeg -re -i why.mp4 -c copy -f rtsp rtsp://wowza:wowza@192.168.7.32:1935/live/mystream
ffplay rtsp://192.168.7.32:1935/live/mystream
推完之前运行可播放，推完后在执行：method DESCRIBE failed: 404 Not Found

-re Read input at native frame rate.
用于模拟(不能用于真实的，否则会引起丢包)数据采集设备(grab) 或实时输入流
ffmpeg默认尽可能快的读取输入数据，此选项让读速度慢到真实的帧率.


ffmpeg_opt.c

白雪公主与猎人.mp4 视频是mpeg-4 而不是h264，结果ffmpeg无法转为ts，何故？

AVInputFormat.read_packet(AVPacket *pkt)  读一个视频帧


解ts时AVPacket -> data是PES包(AUD+SPS+PPS+I)

ffplay.c分析
创建有21个线程

Pthread_frame.c
Pthread_slice.c

用 SDL_CreateThread 创建了 video_thread 和 read_thread

解码过程：
avcodec_decode_video2 
get_video_frame (ffplay.c)
video_thread

七牛:
ffmpeg -re -i /PATH/TO/LOCAL.mp4 -c copy -f flv rtmp://nsqhqt.pub.z1.pili.qiniup.com/readygo/live1?key=4913fac0
ffplay -rtmp_live -1 -rtmp_buffer 3000 rtmp://nsqhqt.live1.z1.pili.qiniucdn.com/readygo/live1

rtmp://nsqhqt.pub.z1.pili.qiniup.com/readygo/CHANNEL68b0134bfbb3494f9a21d49d6921e291?key=80b5f291da078756
rtmp://nsqhqt.live1.z1.pili.qiniucdn.com/readygo/CHANNEL68b0134bfbb3494f9a21d49d6921e291

ffplay http://nsqhqt.hls1.z1.pili.qiniucdn.com/readygo/live1.m3u8

下行服务器：
219.149.55.201
123.159.206.9
58.241.35.136
上行2台：

蓝讯：
上行：rtmp://push-live.cloudpool.com.cn/cdnlives/test1 
下行：rtmp://pull-live.cloudpool.com.cn/cdnlives/test1
APP名称:readygo  
Stream名称:test1，test2，test3，test4

网宿
推 rtmp://ltpush.8686c.com/douniu/12 
拉 rtmp://ltpull.8686c.com/douniu/12

ioctl(VIDIOC_STREAMON): No space left on device
/dev/video3: No space left on device
--
新摄像头取图像
ffmpeg -i /dev/video0  test%d.jpg  与罗技不同，取多张才能找到图像，前几幅一直是黑的
---
avformat/utils.c

======================
avformat_open_input(AVFormatContext **ps, ..., AVDictionary **options)  //ps代表s的p，这个s可外部或内部分配
options 参数怎么设置进去？

avformat_find_stream_info()
注释说用于ts等没有头信息的，但播放mp4，不调用这个也不行，跟踪发现调用前后s结构体多初始化了很多字段

avcodec_decode_video2(avctx, frame, ...); frame->data 地址每次返回都不同，内有缓冲
frame=av_frame_alloc(); 只是分配结构体本身，字段不会初始化。

//内存是1024字节对齐的
bmp = SDL_CreateYUVOverlay(w, h, SDL_YV12_OVERLAY, screen); 

AVPacket pkt = { 0 }; //这种写法

参数系统理解

-profile:v high -level:v 4.0
-profile:a

-framerate (input formats for image2/v4l2v4l2，支持5/10/15/20/25/30，取最接近值)
-r:v
-r[:stream_specifier] fps (input/output,per-stream) Set frame rate

libavdevice/v4l2.c : { "framerate", "set frame rate", {.str = NULL}

libavformat/rawdec.c
{"framerate", "set frame rate", {.str = "25"} }  //default 25
{"pixel_format", "set pixel format", {.str = "yuv420p"}

RAW demuxers  没有封装？//ES流吧
avcodec 和 avformat 各有一个rawdec 

-level 属于哪部分？所有视频格式都有level吗？

v4l2 -list_formats all 罗列摄像头支持的所有分辨率

18.4 libx264, libx264rgb(可以rgb格式输入)

5.5 Video Options
11 Codec Options

关于ffmpeg参数和libx264私有参数

AVOption

movenc.c
.video_codec = CONFIG_LIBX264_ENCODER ?AV_CODEC_ID_H264 : AV_CODEC_ID_MPEG4 //没有libx264 就用mpeg4

anton.mov :-)
视频：sorensen video 3  (svq3)

libx264.c中的x264_profile_names在如下定义：
x264.h: x264_profile_names[] = { "baseline", "main", "high", "high10", "high422", "high444", 0 };

如何播放h264裸流

--enable-librtmp         enable RTMP[E] support via librtmp [no]

ffplay -s 1920*1080 -vcodec yuv4  -f rawvideo why.mp4

codecs 和 formats  都有 rawvideo  

h264  支持SVC  一个视频流里面有多个 layer   包含不同分辨率的图片

cf. x264 --fullhelp    cf. == confer  参见 

x264 [error]: baseline profile doesn't support 4:2:2

av_opt_set_dict2

AVClass *av_class; //用于logging 和 avoptions

看这个 avcodec_open2

最终靠 av_opt_set_defaults 设置各级默认参数
如果cli修改了默认参数，系统走到这里的时候，我猜应该已经改了

#define FF_PROFILE_UNKNOWN -99   在libx264里面就是这样

libx264.c 中的参数设置，调用路径是：
#0  av_opt_set_defaults2
#1  av_opt_set_defaults
#2  avcodec_get_context_defaults3
#3  avcodec_alloc_context3
#4  new_output_stream
#5  new_video_stream
#6  open_output_file
#7  open_files
#8  ffmpeg_parse_options //仅执行一次
#9  main


解析libavcodec/options_table.h 过程：
#0  av_opt_set_defaults2
#1  avcodec_get_context_defaults3
#2  avcodec_alloc_context3
#3  avformat_new_stream
#4  v4l2_read_header
#5  avformat_open_input
#6  open_input_file
#7  open_files
#8  ffmpeg_parse_options
#9  main


疑问：ffmpeg 的profile参数与libx264的profile参数，如何区分。

void av_packet_unref(AVPacket *pkt);

理解buffer ref

解码的时候，AVPacket 是如何传递下去的？

libavcodec里搜索不到 AVFormatContext 和 AVPacketList

side_data
av_freep  注意后面有p，释放空间，同时指针值清零

av_free_packet() 并没有释放AVPacket结构体本身，将buf，data，side_data 指针清零，指向数据释放（AVBuffer 参考数减1）
而AVPacket其他字段保持原有值。
聂：每解完一帧音频视频，用户要手动调用此函数释放（why？）

AVCodecContext 有 refcounted_frames 字段，值为0或1
默认为0，解码器自己调用 av_frame_unref() 释放AVFrame， 但ffplay  ffmpeg 都给设置成了1，那就需手动释放了？（why？）
聂没有设置，他是自动释放。

ffplay.c 中 packet_queue_get() 从AVPacket队列中获取一个

read_thread() 填充AVPacket   video_thread() 消耗AVPacket

我猜对了： avcodec_flush_buffers 在seek时会进入，路径：
#0  avcodec_flush_buffers
#1  get_video_frame
#2  video_thread

ffplay.c中，所有类似 if (pkt == &flush_pkt)的判断，与seek有关

flush_pkt 只是用于比对，不会存入真正的音视频数据，把它插入到链表里面，用于标示位置，方便比较。
如果用一个全局变量达到此目的，需要用信号量保护，而这种方法，本来也有信号量保护那个AVPacket列表，把它插入进入，就利用了原有结构。

seek后，先把AVPacket列表清除，然后插入flush_pkt，如果某个AVPacket正在解码，那么清除也没关系，因为有AVBuffer的ref count机制。

genpts ! 这个参数何意  （-fflags  和  ffplay工具都有这个参数）  从源码看，是用dts产生pts （那为什么还要存储pts呢？）

mp4的pts和dts是怎么计算出来的：

2个AVPacket的列表：ffplay层，这个缓冲用于处理同步，AVFormatContext 里面的，用于处理读数据，和解码数据的速度缓冲，比如读数据卡住了等
2个层面的缓冲，乍一看貌似可以合二为一，实际各层缓冲起不同作用，还没有完全理解。就好比房子的保温层和你的棉衣保温，你能合二为一吗？

参数系统，先按数组初始化，再修改命令行中修改的，而非先把值修改到数组再初始化，那个数组是const的

ffplay -fflags genpts x.mp4 参数控制了 av_read_frame() 的走向

没有B帧的mp4文件的AVPacket  pts一直等于dts，这貌似不合理，没有留出解码时间呀，应该错后一个帧的时间吧！
有B帧的，就跟之前调查过的ts流的i/b/p的pts，dts一致，需要错候2个帧时间才能完全错开。

ffplay.c 里的delay 是每帧的时间长度

40-100ms 为同步容限
#define AV_SYNC_THRESHOLD_MIN 0.04
#define AV_SYNC_THRESHOLD_MAX 0.1


av_gettime_relative()  64bit 相对(relative)微秒数，pc开机从0开始能计58万年


if (lastvp->serial != vp->serial && !redisplay){ //seek时才进入
    is->frame_timer = av_gettime_relative() / 1000000.0;
}


调查一下av_gettime_relative的解析度，pc上连续两次调用，多数数值一样，个别不等的时候相差1(us)


//frame_timer首次在这里初始化，正常情况只进入一次
if (delay > 0 && time - is->frame_timer > AV_SYNC_THRESHOLD_MAX)
    is->frame_timer = time;

redisplay 变量 ，在调整播放窗口大小时会被置1(0,1两个值)


SDL_VIDEOEXPOSE  //视频窗口最小化后再显示

前提假设pc系统时间是绝对准确的

video_refresh解码环节如果 av_usleep(45000.0); //或者40ms以上，会导致整个逻辑打乱无法播放，持续进入retry: 逻辑（形成了循环）
实测，usleep超过35ms 就开始进入retry



23.976  24
29.97


why.mp4 非常标准，4933÷205.746 = 23.976，实际不是24帧
23.976相对24，帧率降低了0.1/%
帧时长： 42 41 42 42 42 41 42 42 41 42 42 42 ... 平均下来是41.708ms
而不是每个帧真的是 41.708ms

NTSC是29.97fps也是30帧降低0.1%（避免干扰）

在 SDL_DisplayYUVOverlay 前面标记时间：
Video_Master:
**2868608256
**2868650336 42080
**2868691313 40977
**2868733331 42018
**2868775299 41968
**2868817336 42037
**2868858322 40986
**2868900312 41990
**2868942313 42001
**2868983324 41011
**2869025315 41991
**2869067314 41999  

Audio_Master:  (默认)
**4323096253
**4323112568 16315
**4323124923 12355
**4323138558 13635
**4323180336 41778
**4323222308 41972
**4323263323 41015
**4323305307 41984
**4323347317 42010
**4323388314 40997
**4323430323 42009


调查花椒直播
抓包分析 800x600 不是rtmp

花椒的h264的参数是？比如I帧间隔
armeabi/libffmpeg.so
armeabi/libjplayer.so
armeabi/libopenh264.so
armeabi/libopenh264jni.so
armeabi/libplayerjni.so
armeabi/libweibosdkcore.so

yy用udp私有协议

七牛：
MediaCodec 从 android api 18 开始加入了 google 的 CTS
这种延迟，如果播放器没有主动丢包，是没法恢复抖动造成的延迟的


bytestream2_get_be16
等函数在：
libavcodec/bytestream.h中定义

rtmp:

-rtmp_buffer 3000 
gen_buffer_time(URLContext *s, RTMPContext *rt)
{
...
拼出数据：00 03 00 00 00 01 00 00 0b b8
这些数据出现在RTMP Body中
    bytestream_put_be16(&p, 3);  
    bytestream_put_be32(&p, rt->stream_id);
    bytestream_put_be32(&p, rt->client_buffer_time);
//执行完这句，可以抓到 发送一个rtmp User Control Message
//Event type: Set Buffer Length
    return rtmp_send_packet(rt, &pkt, 0);
}

-rtmp_live -1

int gen_fcsubscribe_stream()
{

//抓包产生：
//Real Time Messaging Protocol (AMF0 Command FCSubscribe())
//Body: 
//String 'FCSubscribe'
//Number 3
//Null
//String 'live1'

    ff_amf_write_string(&p, "FCSubscribe");
    ff_amf_write_number(&p, ++rt->nb_invokes);
    ff_amf_write_null(&p);
    ff_amf_write_string(&p, subscribe); //subscribe = "live1"

    return rtmp_send_packet(rt, &pkt, 1);
}


---
各种buffer
IO_BUFFER_SIZE 32768  //干什么用的？

packet_queue_put_nullpacket() //干什么用的？
eof时为什么要插入空包？
0722 与-loop参数有关，去除插入空包，视频不能循环播放

ffmpeg压缩摄像头数据，无音频，baseline 640x480 15fps 使用七牛测试地址对，延迟3-4s
而手机liveencoder同样参数，ffplay播放，1s以内,
难道手机压缩比pc还快？不可能吧

libx264 gop默认250，liveencoder gop 30，
gop改为30，-preset ultrafast / superfast 接近liveencoder效果，（时间差在编码速度上）

藏云港：-preset:v ultrafast 竟没报错，:s 就报错
:v是统一处理的，还是独立处理的

可是，怎么说15fps，66ms 也能压缩一帧数据（可以测试一下）这不应该是影响1s还是4s延迟的因素呀！

-x264-params keyint=15  // -x264-params I=15 不行，短选项在x264.c cli中定义，核心库不认。

I帧间隔2s

2个指标：视频打开时间和延迟时间
liveencoder编码，ffplay播放
1 不加nobuffer参数： 5-6s
    加nobuffer参数： 7-8s   也就是说buffer里缓冲了3s数据

2 liveplayer播放：3-4s

3 花椒 3-4s （最快2s，最慢5s）
延迟：

nodemedia  liveplayer: 

0721测试数据： 奇怪，今天的表现整体比昨天（上面数据）好，可能是我把代码切到2.4有关，原来不是2.4
七牛地址测试：
rtmp://nsqhqt.live1.z1.pili.qiniucdn.com/readygo/CHANNEL68b0134bfbb3494f9a21d49d6921e291
不加nobuffer： 黑屏1-3s 延迟5s
加nobuffer：

-probesize 1024 
-formatprobesize 2048
-analyzeduration

压缩15fps 比较好，25fps 好像有问题

ijkplayer 改小probesize rtmp反应快了，但是hls无法播放，因为ts片段需要更多的数据probe，提示：
Failed to open file 'xxxlist.m3u8' or configure filtergraph ，没分析出音视频流
是因为没有正确probe，没有分析出ts里面的音视频流。
对于hls，命令行参数probesize 只能作用于hls层，不能作用于ts层。

看这个 avcodec_open2

可是probesize 2048 播放网络mp4就没事，播ts就不行

音频的pts到底在哪里：

why.mp4  aac (LC)

rtmp时，name是flv

-----------
参数来源2处：OptionDef/AVOption
OptionDef ：用于ffplay/ffmpeg/ffserver命令行，并且给ffmpeg定义的参数不能用于ffplay
如 -filter就不能用于ffplay    

argname 字段是参数的名字
如 -h后：
-r rate    set frame xxx
这里的“rate” 就是argname，告诉你-r后面需要一个参数


明白了一点：OPT_VIDEO | HAS_ARG  | OPT_PERFILE | OPT_OUTPUT
OPT_VIDEO 表示这是视频参数，执行ffmpeg -h时，显示在：
Video options:
-xx xxx
...




cmdutils.c 中关于参数组的概念，比如
ffmpeg -a 1 -b 2 -c 3 -i 1.ts -a 1 -b 2 -i 2.ts -a 3.ts -b 2 -c 3 4.ts
则输入参数有2组，每组分别3个，2个参数，输出参数有2组，每组分别1个，2个参数l


//Fallback for options that are not explicitly handled, these will be parsed through AVOptions.
int opt_default(void *optctx, const char *opt, const char *arg);

ffplay.c 靠
{ "default", HAS_ARG | OPT_AUDIO | OPT_VIDEO | OPT_EXPERT, { .func_arg = opt_default }, "generic catch all option", "" }
引入opt_default函数，
ffmpeg在split_commandline函数中调用 opt_default

他们采用的方法不同，

--
rtmp 延迟与相应速度记录
20150729
pc v2.5  ffplay -probesize 2048 -fflags nobuffer  rtmp://xxx  有时会播不出来，流格式没有检测出来
ijk ffv2.5 使用上面参数也出现黑屏，去掉nobuffer就好了（那次测试情况最好的一次，确实没有加nobuffer）

./ffplay.c:    parse_options(NULL, argc, argv, options, opt_input_file);

parse_options 少数几次调用
多数是 parse_option()
-filter:v 有OPT_SPEC标志位，在parse_option() 被strchr()

int synchronize_audio(VideoState *is, int nb_samples)
{
    int wanted_nb_samples = nb_samples;
	...
    return wanted_nb_samples/2;  //速度加倍
}


设置参数：
opt.h /  dict.h

av_dict_set()  设置字典，此时参数还没设置进去

av_dict_copy() //copy字典结构体

av_dict_get

av_opt_set_dict(void *obj, AVDictionary **options)
将obj不认的opt返回到options中，
param obj a struct whose first element is a pointer to AVClass

param options options to process. This dictionary will be freed and replaced
 *                by a new one containing all options not found in obj.
 *                Of course this new dictionary needs to be freed by caller
 *                with av_dict_free().

 * @return 0 on success, a negative AVERROR if some option was found in obj,
 *         but could not be set.


设置参数的，搜索标志：
AV_OPT_SEARCH_CHILDREN，标明是否设置子对象， 可以搜索子类， 靠AVClass里面的 child_next 函数
AV_OPT_SEARCH_FAKE_OBJ   fake 伪造


// Return next AVOptions-enabled child or NULL
void* (*child_next)(void *obj, void *prev);

/**
 * Return an AVClass corresponding to the next potential
 * AVOptions-enabled child.
 *
 * The difference between child_next and this is that
 * child_next iterates over _already existing_ objects, while
 * child_class_next iterates over _all possible_ children.
 */
const struct AVClass* (*child_class_next)(const struct AVClass *prev);



AVFormatContext
AVCodecContext  分别定义了

format_child_next

avformatcontext 依次检索到：
ff_avio_class
alsa_demuxer_class
dv1394_class
fbdev_class

-----------
理解AVFormatContext.priv_data
AVInputFormat ff_mov_demuxer = {
...
    .priv_data_size = sizeof(MOVContext),
}
在utils.c中分配私有数据 
/* Allocate private data. */


理解AVCodecContext.priv_data   这个值指向之后，是个AVClass * 
priv_data 是具体AVCodec 里面自己定义的一个结构体，比如libx264里面的

int avcodec_open2(AVCodecContext *avctx, const AVCodec *codec, AVDictionary **options);
avctx里面是包含codec的，为啥还要传codec 参数 (avcodec_open2 有详细解释)


PLPlayerKit 


-profile作用于libx264 而非 AVCodecContext 的原因:
avcodec_open2()中，将
    if ((ret = av_opt_set_dict(avctx, &tmp)) < 0)
        goto free_and_end;
提前到priv_data处理之前，就先作用于AVCodecContext了

AVOptions 最后必需以NULL结尾，否则段错误

av_opt_set_defaults()

avcodec 层的 parser

问题： 播放器的缓冲buffer，依据是什么，然后能够播放了的依据又是什么：
ijkplayer 的 ff_ffplay.c:
void ffp_toggle_buffering_l(FFPlayer *ffp, int buffering_on)
{
    VideoState *is = ffp->is;
    if (buffering_on && !is->buffering_on) {
        ALOGD("ffp_toggle_buffering_l: start\n");
        is->buffering_on = 1;
        stream_update_pause_l(ffp);
        ffp_notify_msg1(ffp, FFP_MSG_BUFFERING_START);
    } else if (!buffering_on && is->buffering_on){
        ALOGD("ffp_toggle_buffering_l: end\n");
        is->buffering_on = 0;
        stream_update_pause_l(ffp);
        ffp_notify_msg1(ffp, FFP_MSG_BUFFERING_END);
    }
}

开始推送打印： av_interleaved_write_frame(): Broken pipe  他说开始只允许本地推送，后来修改后就可以了

AVFormatContext.nb_streams  节目中流个数，如:1 video + 2 audio + 1 subtitle = 4，流index从0计
packet_pending 的含义理解了

----
packet_pending:音频有时1个Packet包含多个帧（For audio it may contain several compressed frames），这时，直到解码完成才将packet_pending清零
实测，why.mp4 每个AVPacket只装载一个AAC Frame！
mp4一个包一个aac帧，ts一个AVPacket一个PES包，包含多个AAC帧。
在没有设置noparse参数情况下，avformat会把一个packet拆解为多个只包含一个AAC帧的pa核心数量：4
Intel 酷睿i5 3470 Intel 酷睿i5 3470
线程数量：4cket

或者 --disable-parser=aac 也能阻止拆解

AVPacket is one of the few structs in FFmpeg, whose size is a part of public
 * ABI. Thus it may be allocated on stack

d->queue->serial != d->pkt_serial  开始播放时是不同的，以后就一直都是1，
seek后 d->queue->serial == 2   d->pkt_serial == 1
执行完 packet_queue_get（） d->pkt_serial更新为2
每seek一次，这个数字增加1，  有什么作用呢？



rtmp 音视频缓冲包个数的对应关系，如 67，21   51，16  23.22ms/音频包  67个是1.555s   21个视频帧（帧率15）是1.4s
51是1.184s  16是1.06s  基本对应。

why.mp4 大概205s，所以音频包个数205000/23.22 = 8835个

1s对应音频帧个数：1000/23.22 = 43个
数据具体化了：buffer 里面65个audio帧，实测延时2.33
65个帧的延迟为 23.22*65 = 1.5s 即本地第一帧的时间可再向前推进1.5s，则编码，网络传输花0.8s
网络正常的时候，音频包45个，合1.044s

parse和noparse的区别（猜测）：精确seek，不parse，只能seek到pes边界处

read_frame_internal(){
        if (!st->need_parsing || !st->parser) {
            /* no parsing needed: we just output the packet as is */   //无须拆解
        } else if (st->discard < AVDISCARD_ALL) {
			//需要拆解，把拆出的包存到parse_queue队列中
            if ((ret = parse_packet(s, &cur_pkt, cur_pkt.stream_index)) < 0) 
                return ret;
        } else {
            /* free packet */
            av_free_packet(&cur_pkt);
        }
}

拆解出的AVPacket 存在此队列里
    /**
     * Packets split by the parser get queued here.
     */
    struct AVPacketList *parse_queue;
    struct AVPacketList *parse_queue_end;

 * Some decoders (those marked with CODEC_CAP_DELAY) have a delay between input
 * and output. This means that for some packets they will not immediately
 * produce decoded output and need to be flushed at the end of decoding to get
 * all the decoded data. Flushing is done by calling this function with packets
 * with avpkt->data set to NULL and avpkt->size set to 0 until it stops
 * returning samples. It is safe to flush even those decoders that are not
 * marked with CODEC_CAP_DELAY, then no samples will be returned.

EOF时插入空包是为了把最后B帧挤出来显示

rtmpdump

ffplay -f rawvideo -c:v rawvideo -s 1280*720 -pix_fmt nv12 xx.bat

ffplay test1.flv -vf "rotate=PI/2"

--------


各种时间都以AVStream->time_base为单位

* pts MUST be larger or equal to dts as presentation cannot happen before
* decompression, unless one wants to view hex dumps. Some formats misuse
* the terms dts and pts/cts to mean something different. Such timestamps
* must be converted to true pts/dts before they are stored in AVPacket.

flags 
#define AV_PKT_FLAG_KEY     0x0001 // The packet contains a keyframe
#define AV_PKT_FLAG_CORRUPT 0x0002 // The packet content is corrupted

typedef struct AVPacket {
    AVBufferRef *buf; //参考计数，可能空
    int64_t pts; //Presentation timestamp
    int64_t dts; //Decompression timestamp

    uint8_t *data; //与buf中的指针一致
    int   size;
    int   stream_index;
    int   flags;
    AVPacketSideData *side_data;
    int side_data_elems;
    
    int   duration; // Duration of this packet , 0 if unknown
    int64_t pos;    // byte position in stream, -1 if unknown(本地文件的话就是相对于文件开头的位置)
    int64_t convergence_duration; //解释没看明白
} AVPacket;

-----
-threads  -thread_type 参数

thread_count
thread_type // frame / slice

ffmpeg -i why.mp4 -c:v libx264 -c:a copy why-1.mp4  //默认是"auto"很多线程，cpu接近 400%   5min
ffmpeg -threads auto -i why.mp4 -c:v libx264 -c:a copy why-1.mp4  //等价于上一行
ffmpeg -i why.mp4 -threads 1 -c:v libx264 -c:a copy why-1.mp4 //线程数减少一部分 cpu 100%+  解码虽然多线程，但编码单线程速度是瓶颈，所以cpu率山不去
ffmpeg -threads 1 -i why.mp4 -c:v libx264 -c:a copy why-1.mp4 //cpu 350%+ 
//等价于上一行 
//因 libx264.c 中有 AVCodecDefault x264_defaults = {"threads", AV_STRINGIFY(X264_THREADS_AUTO) }
//X264_THREADS_AUTO 是数字 0 ，AV_STRINGIFY将其转换为 "0"
ffmpeg -threads 1 -i why.mp4 -threads 0 -c:v libx264 -c:a copy why-1.mp4
ffmpeg -threads 1 -i why.mp4 -threads 1 -c:v libx264 -c:a copy why-1.mp4 //线程数只剩一个 cpu 100% 17min
输出参数区域的解析被x264截获，不能识别 auto，如：
ffmpeg -i why.mp4 threads auto -c:v libx264 -c:a copy why-1.mp4 //执行错误

前一个是解码线程数，后一个是编码线程数

之前理解了 x264.c 中的 AVOption是怎样影响命令行参数的，那么
AVCodecDefault x264_defaults又是如何影响的呢？

每个解码器里的 AVCodecDefault 也是会在参数系统里解析的

在后面加 -threads auto 提示：
[AVFilterGraph @ 0x292c8c0] Unable to parse option value "auto"  //需要在avfiltergraph.c中threads组增加auto子项
这又涉及到复杂的参数系统了：
如果 -threads aaa :

[AVFilterGraph @ 0x2405ca0] Unable to parse option value "aaa"   //所以这里可能无关紧要

//打印Eval的时候，说明后面要用eval模块，计算表达式的值
[libx264 @ 0x240a600] [Eval @ 0x7ffff714ce50] Undefined constant or missing '(' in 'aaa'   // '(' 可以引用常量吗
这里的 constant 指的是 PI 这样的内定义
[libx264 @ 0x240a600] Unable to parse option value "aaa"
[libx264 @ 0x240a600] Error setting option threads to value aaa.

./libavutil/opt.c:            av_log(obj, AV_LOG_ERROR, "Error setting option %s to value %s.\n", t->key, t->value);


-threads au 为啥 AVFilterGraph模块没解析出来没事，libx264模块就导致程序退出呢？
[AVFilterGraph @ 0x3b38ce0] [Eval @ 0x7ffe5ad2c560] Undefined constant or missing '(' in 'au'
[AVFilterGraph @ 0x3b38ce0] Unable to parse option value "au"
[libx264 @ 0x3ab2660] [Eval @ 0x7ffe5ad2c4d0] Undefined constant or missing '(' in 'au'
[libx264 @ 0x3ab2660] Unable to parse option value "au"
[libx264 @ 0x3ab2660] Error setting option threads to value au.  程序退出

因为codec模块采用类似：
    if ((ret = av_opt_set_dict(s, &tmp)) < 0)
        goto fail;
这样的逻辑，当检测到参数比如-threads 但是au没解析出来返回值就<0了，而avfiltergraph模块的设置是:
configure_filtergraph()函数中：
e = av_dict_get(ost->encoder_opts, "threads", NULL, 0);
if (e)
    av_opt_set(fg->graph, "threads", e->value, 0);  //这里的返回值已经<0 但是没有处理

-threads 有多个模块都有

eval.c (libavutil) 

9 Expression Evaluation

-threads 'abs(-2)'  // OK
-threads "pow(1,2)" // OK

-threads 'absd(-2)'  /  -threads "absd(-2)"
Unknown function in 'absd(0)'


关于 -threads 同时在avcodec 和 avfiltergraph 2个模块，按照之前的理解，设置参数后，第一个模块捕获到，后面的模块就没了
所以我认为，参数成功被 avfiltergraph 捕获到，解码器就没有了，而实际测试并非如此，
实际是，只有调用 av_opt_set_dict() 才是这样，有方法让这个参数在2个模块都有效。


参数组：
这里说的参数组，是说某个参数，后面定义各种 alias参数，注意，主和附条目，最后一个字段都要一样，也就是说，主条目
第一字段和最后字段一样


滤波器是怎么对接到系统中的：
libavfilter/vf_edgedetect.c
shi
av_err2str()  转化错误码到字符串

aac_adtstoasc

AVFormatContext 的priv_data字段

(gdb) p *(AVClass*)child
(gdb) p **(AVClass**)child

av_opt_find2() 递归

参数的递进寻找：

format_child_next()
avformatcontext: 包括自己的avclass(这里的递归逻辑比较邪门)  s->priv_data 的avclass 和 s->pb 的AVclass

AVClass:
.item_name  = av_default_item_name, 默认av_log打印类的类名， 当然这里可以自定义了，又明白一点

compute_pkt_fields2() 有 st->cur_dts > pkt->dts
Application provided invalid, non monotonically increasing dts to muxer in stream
收取udp ts流，输入设为nopts，输出的AVPacket 的pts从0开始，到最大值33bit会折回。而cur_dts持续增加，在边界处，错误条件被触发。
bug?

xuss:
播放RTMP直播偶发只有音频，视频无法解码或需时较长问题：
RTMP完成握手和信息交换后发送metadata，videodata，audiodata，AVC编码应优先发送SPS，PPS
抓包发现服务器没有或延迟或部分发送sps，pps


extradata 问题：
why.mp4  41字节 与mp4文件 avc1.avcC 一致，那里显示49字节，减去4字节长度+avcC 4个字幕剩41字节

前5字节没用，包含版本信息（顾），stagefright要检测第一个字节必须为0x1
h264_extradata_to_annexb()
{
const uint8_t *extradata = avctx->extradata + 4;
int length_size = (*extradata++ & 0x3) + 1; // 2字节表示长度
运行到最后，得到类似下面ts的extradata，但没有aud
}
0x1, 0x64, 0x0, 0x28, 0xff, 0xe1,-- 0x0, 0x19（后面25字节）, ---0x67, 0x64, 0x0, 0x28, 0xac, 0x24, 0x88, 0x7, 0x80, 0x22, 0x7e, 0x5c, 
0x4, 0x40, 0x0, 0x00, 0x1f, 0x40, 0x0, 0x5, 0xda, 0x83, 0xc6, 0xc, 0xa8, ---- 0x1, 0x0, 0x5（后面5字节）,---- 0x68, 0xee, 0x32, 0xc8, 0xb0

这里没有，也不需要 AUD信息

转换为ts： 44字节
0x0, 0x0, 0x0, 0x1, +++0x9, 0xf0(2字节AUD), +++ 0x0, 0x0, 0x0, 0x1, -----0x67, 0x64, 0x0, 0x28, 0xac, 0x24, 0x88, 0x7, 0x80, 0x22, 0x7e, 0x5c, 0x4, 0x40, 0x0, 0x0, 0x1f, 0x40, 0x0, 0x5, 0xda, 0x83, 0xc6, 0xc, 0xa8,--- 0x0, 0x0,  0x0, 0x1, --- 0x68, 0xee, 0x32, 0xc8, 0xb0

???
--------------------------
20160414  gu
收到avcc 转换成annexb
AVcc extradata:
0x1 0x64 0x0 0x1f 0xff 0xe1 0x0 0x14 0x27 0x64 0x0 0x1f 0xac 0x13 0x14 0x50 0x32 0xf 0x69 0xb8 0x10 0x10 0x10 0x36 0x82 0x21 0x19 0x60 0x1 0x0 0x4 0x28 0xef 0x9 0xcb 0x2 0x0 0x0 0x0

annexBHeader:
0x00 0x00 0x00 0x01 0x27 0x64 0x00 0x1f 0xac 0x13 0x14 0x50 0x32 0xf 0x69 0xb8 0x10 0x10 0x10 0x36 0x82 0x21 0x19 0x60 00x0 0x00 0x00 0x01 0x28 0xef 0x9 0xcb

--------------------------

"libstagefright_h264" 在这里，libstagefright.c 就是一个h264解码器，而不是android里面的stagefright多媒体架构

#0  ff_h264_decode_init (执行到这里的时候已经填充好了)
#1  avcodec_open2
#2  avformat_find_stream_info

ffmpeg 2.8 

Stagefright_init()
{
    if (!avctx->extradata || !avctx->extradata_size || avctx->extradata[0] != 1)
        return -1;
}

手动填充AVPacket结构体

ffmpeg 用了binder与stagefright解码

ffplay 直接播放ES流文件，annexb间隔，不论扩展名，都能识别
打印*s->iformat：name = 0x13d1650 "h264", long_name = 0x13d1655 "raw H.264 video"

那么libx264 编码出来的es流，却有一堆编码器信息的字符串，这存在哪里呢？

h264_extradata_to_annexb

AAC:
https://trac.ffmpeg.org/wiki/Encode/AAC

-----
聂志军：
mp4 转 ts，切hls，有的段不能播，他说后面的没有pps和sps，但实际测试有，是其他问题。

----
ffmpeg识别.yuv扩展名，默认 yuv420p

mp3 encoder: libmp3lame / libshine
解码自带:
libavcodec/mpegaudiodec_fixed.c 
libavcodec/mpegaudiodec_template.c
--disable-decoder=mp3 影响
./config.h:#define CONFIG_MP3_DECODER 0


liveencoder
nodemedia.cn
libLiveEncoder.so  2.4M
断网续传， rtmp的快速响应


ffplay 71.rm 
  Metadata:
    File ID         : 2ffc19f3-d345-84bc-ad79-7ccec9249a33
    Description     :  //可加描述
    title           : 71 b
    author          : Jay Edwards
  Duration: 01:27:46.29, start: 0.000000, bitrate: 354 kb/s
    Stream #0:0: Audio: cook (cook / 0x6B6F6F63), 44100 Hz, stereo, fltp, 64 kb/s
    Stream #0:1: Video: rv40 (RV40 / 0x30345652), yuv420p, 320x240, 285 kb/s, 29.97 fps, 

16级音量，1-255是振幅，要让振幅等比（类似db概念）
255^(1/16) = 1.413867662
1 1.4 1.9 2.8 4 5.6 7.9 11.3 16 22.6 32 45 63.8 90 127 180 255

----------
pcm_s16be codec not supported in WAVE format

ffmpeg -i Lemon.wav   -f s16be Lemon_be (不要迷糊)
(pcm_s16le (native) -> pcm_s16be (native))   //native 什么含义？


Annex B: NAL 靠 0x00 0x00 0x00 0x01分隔，为了找到NAL起始, 需要读取流的每个字节判断
AVCC: NAL靠4字节size分隔

做循环发送，不想关闭再打开文件。参考ffplay.c中stream_seek的原理，
其实，循环就是seek，只不过是从文件最后seek到了最开始，不是简单的fseek()

---------Log系统-------------
guwb  yuvj420p问题
能正常播放的提交点：
avcodec/h264:Execute errorconcealment before marking the frame
c4765a41b956f15b8557...
这个打印：
[swscaler @ 0x7f6f9c0a18e0] deprecated pixel format used, make sure you did set range correctly

UVI

这个流分上下2个slice，解码不正常的时候，上半部分不正常，下半部分正常

concealing 750 DC, 750 AC, 750 MV errors in I frame  (错误隐藏框架分析)

20160608发现问题，我提示是yuvj格式，到我发现分上下2屏，到0623靠他自己突破关键：
2个slice在avcc里是分2次传过来的，被他分在2个nalu中，这解释了为啥开始有2个I帧。为啥ffplay错误时，一半正常一半不正常
上文750，正好是半屏的宏块个数，每个slice（半屏）是一个单独NALU，需要一块送入解码器，前面追加PPS+SPS，即：PPS+SPS+NALU(slice1)+NALU(slice2) 
一起送入rockchip解码器，正常

elecard stream analyzer看，2个I帧slice编号都是0
第一次在elecard analyzer看到多slice时，当时根本没意识到他们在单独nal里，潜意识里还认为多slice在一个nal中，
甚至elecard streaming eye看到2个I帧的时候还没反应过来 

说明多个slice分为多个nalu是符合标准的，rockchip芯片不能解，freescale能解，解码器兼容性

搜索 slice NAL

---
2路音频混合算法：

1路立体声，怎样转换为2个单声道

video/mpeg2-I.mpg 扩展名随便改，还是能识别，怎么做到的？   检查格式？
上来 00 00 01 b3
还有00 00 01 b5
00 00 01 b8  这些都是mpeg2的格式

--
libavdevice/dshow.c
算帧率 1e7 / vcaps->MaxFrameInterval,
1e7 是1x10^7

====== yup =======
libyuv: is an open source project that includes YUV scaling and conversion functionality
git clone https://chromium.googlesource.com/libyuv/libyuv
* Optimized for Neon on Arm.
* Optimized for SSE2/SSSE3/AVX2 on x86/x64.
* Rotate by 90/180/270 degrees to adjust for mobile devices in portrait mode.

原工程是gyp难编译

yuv格式复杂，ffmpeg/openmax/fourcc.org/yuv.php 除常见还有很多种
yuv分：
packed : yuv在一起存储 （ffmpeg的 yuyv422）
planar : 各分量单独存储

Chroma-interleaved:

https://msdn.microsoft.com/en-us/library/aa904813%28VS.80%29.aspx


NN11

YUV420P，Y，U，V三个分量都是平面格式，分为I420和YV12。I420格式和YV12格式的不同处在U平面和V平面的位置不同。在I420格式中，U平面紧跟在Y平面之后，然后才是V平面（即：YUV）；但YV12则是相反（即：YVU）。
YUV420SP: Y平面，UV打包，即NV12(FourCC)

//http://www.fourcc.org/yuv.php#Planar%20YUV%20Formats 有解释
NV12: 8-bit Y plane followed by an interleaved U/V plane with 2x2 subsampling
      YYYYYYYY UVUV     =>YUV420SP //Semi-Planar
NV21: As NV12 with U and V reversed in the interleaved plane
      YYYYYYYY VUVU     =>YUV420SP //Semi-Planar
I420: 8 bit Y plane followed by 8 bit 2x2 subsampled U and V planes
      YYYYYYYY UU VV    =>YUV420P
YV12: 8 bit Y plane followed by 8 bit 2x2 subsampled V and U planes.(与I420比，UV顺序反)
      YYYYYYYY VV UU

猜测：NV12，NV的V代表没有V分量，对应yuv420中的0，当然实际并非没有V，12代表12bit/pixel，NV21，21是12倒过来，代表UV变VU

Planar \ Semi-Planar \ Interleaved

yuv420p  p: planar(平面的，平坦的)


ffmpeg:
AV_PIX_FMT_YUV420P

SDL_YV12_OVERLAY 与 YUV420P  u v 分量顺序是反的

yuv 420  u/v 分量0x80是灰色

OpenMAX中：


ffmpeg:
yuv420p
yuv422p
yuv444p
yuv410p
yuv411p
yuv440p
yuyv422  //yu yv yu yv (突然明白yuv420没法packet，因为每个packet无法保证等长，因为是12bit/pixel)
yvyu422  //yv yu yv yu

yuv444p  //yyyy...  uuuu... vvvv...
//每像素12bit，与420相似
nv12
nv21

nv16


bmp转yuv420p 与nv21/12 除了存储顺序不同，yuv分量的值也略不同，用了不同的公式 ITU BT.601  BT.709 ?
sws_scale()


//这组，每个分量用16bit表示
yuv420p16be
...
yuv444p16le

实验：
640x480 红色rgb

-------
yuvj420p 与yuv420p 格式一致
yuvj使用Jpeg转换公式，范围0-255，
yuv 范围16-240，
yuv可以当yuvj使用，但yuvj当yuv，需要按比例缩放


    [AV_PIX_FMT_YUVJ420P] = {
        .name = "yuvj420p",
        .nb_components = 3,
        .log2_chroma_w = 1,
        .log2_chroma_h = 1,
        .comp = {
            { 0, 1, 0, 0, 8, 0, 7, 1 },        /* Y */
            { 1, 1, 0, 0, 8, 0, 7, 1 },        /* U */
            { 2, 1, 0, 0, 8, 0, 7, 1 },        /* V */
        },
        .flags = AV_PIX_FMT_FLAG_PLANAR,


    [AV_PIX_FMT_YUV420P] = {
        .name = "yuv420p",
        .nb_components = 3,
        .log2_chroma_w = 1,
        .log2_chroma_h = 1,
        .comp = {
            { 0, 1, 0, 0, 8, 0, 7, 1 },        /* Y */
            { 1, 1, 0, 0, 8, 0, 7, 1 },        /* U */
            { 2, 1, 0, 0, 8, 0, 7, 1 },        /* V */
        },
        .flags = AV_PIX_FMT_FLAG_PLANAR,

-- mjpeg --
ffplay -input_format mjpeg /dev/video0
ffplay -list_formats all   /dev/video0

取出的数据命名为 .mjpeg 用ffplay即可播放

